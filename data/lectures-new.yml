-
  Title: "1. Black Box Machine Learning"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/01.black-box-ML.pdf"
  Video: "https://www.youtube.com/embed/MsD28INtSv8"
  Summary: "<p>With the abundance of well-documented machine learning (ML) libraries, programmers can now \"do\" some ML, without any understanding of how things are working.  And we'll encourage such \"black box\" machine learning... just so long as you follow the procedures described in this lecture. To make proper use of ML libraries, you need to be conversant in the basic vocabulary, concepts, and workflows that underlie ML. We'll introduce the standard ML problem types (classification and regression) and discuss prediction functions, feature extraction, learning algorithms, performance evaluation, cross-validation, sample bias, nonstationarity, overfitting, and hyperparameter tuning.</p><p>If you're already familiar with standard machine learning practice, you can skip this lecture.</p>"
#  Notes:
#    - {"Géron's Machine Learning Landscape": "https://github.com/ageron/handson-ml/blob/master/01_the_machine_learning_landscape.ipynb"}
#    - {"Géron's End-to-End Machine Learning": "https://github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb"}
  References:
    - "Géron Ch 1,2"
    - {Provost and Fawcett book: "http://www.data-science-for-biz.com/DSB/Home.html"}
-
  Title: "2. Case Study: Churn Prediction"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/churn.pdf"
  Video: "https://www.youtube.com/embed/kE_t3Mm8Z50"
  Summary: "<p>We have an interactive discussion about how to reformulate a real and subtly complicated business problem as a formal machine learning problem.  The real goal isn't so much to solve the problem, as to convey the point that properly mapping your business problem to a machine learning problem is both extremely important and often quite challenging.  This course doesn't dwell on how to do this mapping, though see Provost and Fawcett's book in the references.</p>"
  References:
    - {"KDD Cup 2009: Customer relationship prediction": "http://www.kdd.org/kdd-cup/view/kdd-cup-2009"}
    - {Provost and Fawcett book: "http://www.data-science-for-biz.com/DSB/Home.html"}
-
  Title: "3. Introduction to Statistical Learning Theory"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/02a.intro-stat-learning-theory.pdf"
  Video: "https://www.youtube.com/embed/rqJ8SrnmWu0"
  Summary: "This is where our \"deep study\" of machine learning begins. We introduce some of the core building blocks and concepts that we will use throughout the remainder of this course: input space, action space, outcome space, prediction functions, loss functions, and hypothesis spaces.  We present our first machine learning method: empirical risk minimization.  We also highlight the issue of overfitting, which may occur when we find the empirical risk minimizer over too large a hypothesis space."
  Notes:
    - {Conditional Expectations: "https://davidrosenberg.github.io/mlcourse/Notes/conditional-expectations.pdf"}
  CChecks:
    - {SLT and SGD Concept Check Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check.pdf"}
    - {SLT and SGD Concept Check Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check_sol.pdf"}
    - {"Homework 1: §3": "#assignment-homework-1"}
-
  Title: "4. Stochastic Gradient Descent"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/02b.SGD.pdf"
  Video: "https://www.youtube.com/embed/5TZww5bTROE"
  Summary: "A recurring theme in machine learning is that we formulate learning problems as optimization problems.  Empirical risk minimization was our first example of this.  To do learning, we need to do optimization. In this lecture we cover stochastic gradient descent, which is today's standard optimization method for large-scale machine learning problems."
  References:
    - {"Bottou's SGD Tricks": "http://leon.bottou.org/papers/bottou-tricks-2012"}
    - {"Barnes \"Matrix Differentiation\" notes": "http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf"}
  Notes:
    - {Directional Derivatives and Approximation (Short): "https://davidrosenberg.github.io/mlcourse/Notes/directional-derivative.pdf"}
    - {Gradients and Directional Derivatives: "https://davidrosenberg.github.io/mlcourse/Labs/1-gradients-Notes_sol.pdf"}
#    - {Gradient Descent Demo (ipynb): "https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/gd_fixed_and_backtracking.ipynb"}
  CChecks:
    - {SLT and SGD Concept Check Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check.pdf"}
    - {SLT and SGD Concept Check Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/1-Lec-Check_sol.pdf"}
    - {"Homework 1: §2": "#assignment-homework-1"}
    - {"Homework 2: §1,2": "#assignment-homework-2"}
-
  Title: "5. Excess Risk Decomposition"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/02c.excess-risk-decomposition.pdf"
  Video: "https://www.youtube.com/embed/YA_CE9jat4I"
  Summary: "We introduce the notions of approximation error, estimation error, and optimization error. While these concepts usually show up in more advanced courses, they will help us frame our understanding of the tradeoffs between hypothesis space choice, data set size, and optimization run times. In particular, these concepts will help us understand why \"better\" optimization methods (such as quasi-Newton methods) may not find prediction functions that generalize better, despite finding better optima."
  #  (Note: This is independent of the more recent observations that stochastic gradient methods often generalize _better_ than batch gradient methods in neural network settings.)"
  CChecks:
    - {Excess Risk and L1/L2 Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-Lec-Check.pdf"}
    - {Excess Risk and L1/L2 Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-Lec-Check_sol.pdf"}
-
  Title: "6. L1 and L2 regularization"
  Video: "https://www.youtube.com/embed/d6XDOS4btck"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/03a.L1L2-regularization.pdf"
  Summary: "<p>We introduce \"regularization\", our main defense against overfitting. We discuss the equivalence of the penalization and constraint forms of regularization (see <a href=\"#assignment-homework-4\">Hwk 4 Problem 8</a>), and we introduce L1 and L2 regularization, the two most important forms of regularization for linear models.  When L1 and L2 regularization are applied to linear least squares, we get \"lasso\" and \"ridge\" regression, respectively. We compare the \"regularization paths\" for lasso and ridge regression, and give a geometric argument for why lasso often gives \"sparse\" solutions. Finally, we present \"coordinate descent\", our second major approach to optimization.  When applied to the lasso objective function, coordinate descent takes a particularly clean form and is known as the \"shooting algorithm\".</p>"
  CChecks:
    - {Excess Risk and L1/L2 Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-Lec-Check.pdf"}
    - {Excess Risk and L1/L2 Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-Lec-Check_sol.pdf"}
    - {"Homework 2": "#assignment-homework-2"}
  Notes:
    - {Completing the Square: "https://davidrosenberg.github.io/mlcourse/Notes/completing-the-square.pdf"}
    - {Lasso Lecture Prep Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-PreLec-Check.pdf"}
    - {Lasso Lecture Prep Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/2-PreLec-Check_sol.pdf"}
  References:
    - "HTF 3.4" 
-
  Title: "7. Lasso, Ridge, and Elastic Net"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/03b.elastic-net.pdf"
  Video: "https://www.youtube.com/embed/KIoz_aa1ed4"
  Summary: "<p>We continue our discussion of ridge and lasso regression by focusing on the case of correlated features, which is a common occurrence in machine learning practice. We will see that ridge solutions tend to spread weight equally among highly correlated features, while lasso solutions may be unstable in the case of highly correlated features. Finally, we introduce the \"elastic net\", a combination of L1 and L2 regularization, which ameliorates the instability of L1 while still allowing for sparsity in the solution. (Credit to Brett Bernstein for the excellent graphics.)"
  Notes:
    - {Lasso and Elastic Net (ipynb): "https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Lasso%20and%20Elastic%20Net/lasso_and_elastic_net.ipynb"}
    - {Elastic Net correlation theorem: "https://davidrosenberg.github.io/mlcourse/Notes/elastic-net-theorem.pdf"}
  CChecks:
    - {"Homework 2: §4.2,5": "#assignment-homework-2"}
  References:
    - {"Zou and Hastie's Elastic Net Paper (2005)": "https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf"}
    - {'Mairal, Bach, and Ponce on Sparse Modeling': "https://arxiv.org/pdf/1411.3230v2.pdf"}
-
  Title: "8. Loss Functions for Regression and Classification"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/04a.loss-functions.pdf"
  Video: "https://www.youtube.com/embed/1oi_Mwozj5w"
  Summary: "We start by discussing absolute loss and Huber loss.  We consider them as alternatives to the square loss that are more robust to outliers.  Next, we introduce our approach to the classification setting, introducing the notions of score, margin, and margin-based loss functions. We discuss basic properties of the hinge loss (i.e SVM loss), logistic loss, and the square loss, considered as margin-based losses.  The interplay between the loss function we use for training and the properties of the prediction function we end up with is a theme we will return to several times during the course."
  CChecks:
    - {"Homework 3: §2,3": "#assignment-homework-3"}
    - {"Homework 5: §2": "#assignment-homework-5"}

-
  Title: "9. Lagrangian Duality and Convex Optimization"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/04b.convex-optimization.pdf"
  Summary: "<p>We introduce the basics of convex optimization and Lagrangian duality. We discuss weak and strong duality, Slater's constraint qualifications, and we derive the complementary slackness conditions. As far as this course is concerned, there are really only two reasons for discussing Lagrangian duality: 1) The complementary slackness conditions will imply that SVM solutions are \"sparse in the data\" (<a href=\"#lecture-10-support-vector-machines\">next lecture</a>), which has important practical implications for the kernelized SVMs (see the <a href=\"#lecture-13-kernel-methods\">kernel methods lecture</a>). 2) Strong duality is a sufficient condition for the equivalence between the penalty and constraint forms of regularization (see <a href=\"#assignment-homework-4\">Hwk 4 Problem 8</a>).</p>
  <p>This mathematically intense lecture may be safely skipped.</p>"
  Video: "https://www.youtube.com/embed/thuYiebq1cE"
  Notes:
    - {Pre-lecture warmup for SVM and Lagrangians: "https://davidrosenberg.github.io/mlcourse/Notes/svm-lecture-prep.pdf"}
    - {Extreme Abridgment of BV: "https://davidrosenberg.github.io/mlcourse/Notes/convex-optimization.pdf"}
    - {Lagrangian Duality (10-minute summary)): "https://davidrosenberg.github.io/mlcourse/Lectures/04d.lagrangian-duality-in-ten-minutes.pdf"}
  CChecks:
    - {Subgradients and Lagrangian Duality Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/4-Lec-Check.pdf"}
    - {Subgradients and Lagrangian Duality Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/4-Lec-Check_sol.pdf"}
    - {"Homework 4: §8": "#assignment-homework-4"}
  References:
-
  Title: "10. Support Vector Machines"
  Summary: "<p>We define the soft-margin support vector machine (SVM) directly in terms of its objective function (L2-regularized, hinge loss minimization over a linear hypothesis space).  Using our knowledge of Lagrangian duality, we find a dual form of the SVM problem, apply the complementary slackness conditions, and derive some interesting insights into the connection between \"support vectors\" and margin. Read the \"SVM Insights from Duality\" in the Notes below for a high-level view of this mathematically dense lecture.</p>
<details>
  <p><summary>More...</summary>Notably absent from the lecture is the hard-margin SVM and its standard geometric derivation. Although the derivation is fun, since we start from the simple and visually appealing idea of maximizing the \"geometric margin\", the hard-margin SVM is rarely useful in practice, as it requires separable data, which precludes any datasets with repeated inputs and label noise. One fixes this by introducing \"slack\" variables, which leads to a formulation equivalent to the soft-margin SVM we present. Once we introduce slack variables, I've personally found the interpretation in terms of maximizing the margin to be much hazier, and I find understanding the SVM in terms of \"just\" a particular loss function and a particular regularization to be much more useful for understanding its properties. That said, Brett Bernstein gives a very nice development of the geometric approach to the SVM, which is linked in the References below. At the very least, it's a great exercise in basic linear algebra.</p></details>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/04c.SVM.pdf"
  Video: "https://www.youtube.com/embed/9zi6-RjlYrU"
  Notes:
    - {Pre-lecture warmup for SVM and Lagrangians: "https://davidrosenberg.github.io/mlcourse/Notes/svm-lecture-prep.pdf"}
    - {Support Vector Machines: "https://davidrosenberg.github.io/mlcourse/Notes/svm-notes.pdf"}
    - {SVM Insights from Duality: "https://davidrosenberg.github.io/mlcourse/Notes/SVM-main-points.pdf"}
  CChecks:
    - {"Homework 3": "#assignment-homework-3"}
    - {"Homework 4: §4": "#assignment-homework-4"}
  References:
    - {"Geometric Derivation of SVMs": "https://davidrosenberg.github.io/mlcourse/Labs/3-SVM-Notes_sol.pdf"}
    - {"Note on the Uniqueness of SVMs": "https://davidrosenberg.github.io/mlcourse/Labs/UniquenessOfSVM.pdf"}
#    - {"Andrew Ng's CS229 SVM Notes": "http://cs229.stanford.edu/notes/cs229-notes3.pdf"}
#    - "HTF 12.2.1 - 12.2.2"
-
  Title: "11. Subgradient Descent"
  Summary: "<p>Neither the lasso nor the SVM objective function is differentiable, and we had to do some work for each to optimize with gradient-based methods. It turns out, however, that gradient descent will essentially work in these situations, so long as you're careful about handling the non-differentiable points. To this end, we introduce \"subgradient descent\", and we show the surprising result that, even though the objective value may not decrease with each step, every step brings us closer to the minimizer.</p> 
<p>This mathematically intense lecture may be safely skipped.</p>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/05a.subgradient-descent.pdf"
  Video: "https://www.youtube.com/embed/jYtCiV1aP44"
  Notes:
    - {"Subgradients": "https://davidrosenberg.github.io/mlcourse/Labs/4-Subgradients-Notes_sol.pdf"}
  CChecks:
    - {Subgradients and Lagrangian Duality Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/4-Lec-Check.pdf"}
    - {Subgradients and Lagrangian Duality Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/4-Lec-Check_sol.pdf"}
    - {"Homework 3": "#assignment-homework-3"}
  References:
    - {"Boyd's subgradient notes": "http://web.stanford.edu/class/ee364b/lectures.html"}
-
  Title: "12. Feature Extraction"
  Summary: "<p>When using linear hypothesis spaces, one needs to encode explicitly any nonlinear dependencies on the input as features. In this lecture we discuss various strategies for creating features. Much of this material is taken, with permission, from Percy Liang's CS221 course at Stanford.</p>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/05b.features.pdf"
  Video: "https://www.youtube.com/embed/gmli6EyiNRw"
  Notes:
    - {Simplest Example: "https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Features/simple_feature_transformations.ipynb"}
    - {Ingesting text with BOW: "https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Features/test_BOW.ipynb"}
    - {Polynomial features: "https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Features/polynomial_feature_comparison.ipynb"}
    - {Vector quantization with k-means: "https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/Features/vector_quantization.ipynb"}
  CChecks:
    - {"Homework 3: §6,7,8": "#assignment-homework-3"}
  References:
    - {Feature Engineering for Machine Learning by Casari and Zheng: "http://shop.oreilly.com/product/0636920049081.do"}
-
  Title: "13. Kernel Methods"
  Summary: "<p>With linear methods, we may need a whole lot of features to get a hypothesis space that's expressive enough to fit our data -- there can be orders of magnitude more features than training examples. While regularization can control overfitting, having a huge number of features can make things computationally very difficult, if handled naively.  For objective functions of a particular general form, which includes ridge regression and SVMs but not lasso regression, we can \"kernelize\", which can allow significant speedups in certain situations.  In fact, with the \"kernel trick\", we can even use an infinite-dimensional feature space at a computational cost that depends primarily on the training set size.</p>
<details><p><summary>More...</summary>In more detail, it turns out that even when the optimal parameter vector we're searching for lives in a very high-dimensional vector space (dimension being the number of features), a basic linear algebra argument shows that for certain objective functions, the optimal parameter vector lives in a subspace spanned by the training input vectors. Thus, when we have more features than training points, we may be better off restricting our search to the lower-dimensional subspace spanned by training inputs. We can do this by an easy reparameterization of the objective function. This result is referred to as the \"representer theorem\", and its proof can be given on one slide.</p>
  <p>After reparameterization, we'll find that the objective function depends on the data only through the Gram matrix, or \"kernel matrix\", which contains the dot products between all pairs of training feature vectors. This is where things get interesting a second time: Suppose f is our featurization function. Sometimes the dot product between two feature vectors f(x) and f(x') can be computed much more efficiently than multiplying together corresponding features and summing. In such a situation, we write the dot products in terms of the \"kernel function\": k(x,x')=〈f(x),f(x')〉, which we hope to compute much more quickly than O(d), where d is the dimension of the feature space. The essence of a \"kernel method\" is to use this \"kernel trick\" together with the reparameterization described above.  This allows one to use huge (even infinite-dimensional) feature spaces with a computational burden that depends primarily on the size of your training set. In practice, it's useful for small and medium-sized datasets for which computing the kernel matrix is tractable. Scaling kernel methods to large data sets is still an active area of research.<p></details>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/06a.kernel-methods.pdf"
  Video: "https://www.youtube.com/embed/m1otj-SdwYw"
  References:
    - "SSBD Chapter 16"
    - {"A Survey of Kernels for Structured Data": "http://homepages.rpi.edu/~bennek/class/mmld/papers/p49-gartner.pdf"}
  CChecks:
    - {Kernel Concept Check Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/5-Lab-Check.pdf"}
    - {Kernel Concept Check Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/5-Lab-Check_sol.pdf"}
    - {"Homework 4": "#assignment-homework-4"}
-
#: zero-information prediction function, single-feature prediction functions, regularized linear models, and controversially an \"oracle\" model that is trained on your validation data, to get an idea of the upper bound of performance for your learning method.
  Title: "14. Performance Evaluation"
  Summary: "<p>This is our second \"black-box\" machine learning lecture. We start by discussing various models that you should almost ways build for your data, to use as baselines and performance sanity checks. From there we focus primarily on evaluating classifier performance.  We define a whole slew of performance statistics used in practice (precision, recall, F1, etc.). We also discuss the fact that most classifiers provide a numeric score, and if you need to make a hard classification, you should tune your threshold to optimize the performance metric of importance to you, rather than just using the default (typically 0 or 0.5). We also discuss the various performance curves you'll see in practice: precision/recall, ROC, and (my personal favorite) lift curves.</p>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/06b.classifier-performance.pdf"
  Video: "https://www.youtube.com/embed/xMyAL0C6cPY"
  References:
    - Provost and Fawcett book
-
  Title: "15. \"CitySense\": Probabilistic Modeling for Unusual Behavior Detection"
#  <p>In this lecture, we take a \"bare hands\" approach to building these conditional probability models. While I consider these techniques to be very useful in practice, if you're eager to get to some mathematics, you may safely skip this lecture.</p>"
  Summary: "<p>So far we have studied the regression setting, for which our predictions (i.e. \"actions\") are real-valued, as well as the classification setting, for which our score functions also produce real values. With this lecture, we begin our consideration of \"conditional probability models\", in which the predictions are probability distributions over possible outcomes. We motivate these models by discussion of the \"CitySense\" problem, in which we want to predict the probability distribution for the number of taxicab dropoffs at each street corner, at different times of the week. Given this model, we can then determine, in real-time, how \"unusual\" the amount of behavior is at various parts of the city, and thereby help you find the secret parties, which is of course the ultimate goal of machine learning.</p>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/08a.citysense.pdf"
  Video: "https://www.youtube.com/embed/6nolrvzXiE4"
  References:
    - {"CitySense: multiscale space time clustering of GPS points and trajectories": http://www1.cs.columbia.edu/~jebara/papers/CitySense.JSM2009.pdf}
-
  Title: "16. Maximum Likelihood Estimation"
  Summary: "In empirical risk minimization, we minimize the average loss on a training set. If our prediction functions are producing probability distributions, what loss functions will give reasonable performance measures? In this lecture, we discuss \"likelihood\", one of the most popular performance measures for distributions. We temporarily leave aside the conditional probability modeling problem, and focus on the simpler problem of fitting an unconditional probability model to data. We can use \"maximum likelihood\" to fit both parametric and nonparametric models. Once we have developed a collection of candidate probability distributions on training data, we select the best one by choosing the model that has highest \"hold-out likelihood\", i.e. likelihood on validation data."
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/08b.MLE.pdf"
  Video: "https://www.youtube.com/embed/ec_5vvxW7fE"
  CChecks:
    - {"Homework 5: §7,8": "#assignment-homework-5"}
-
  Title: "17. Conditional Probability Models"
  Summary: "In this lecture we consider prediction functions that produce distributions from a parametric family of distributions. We restrict to the case of linear models, though later in the course we will show how to make nonlinear versions using gradient boosting and neural networks. We develop the technique through four examples: Bernoulli regression (logistic regression being a special case), Poisson regression, Gaussian regression, and multinomial logistic regression (our first multiclass method). We conclude by connecting this maximum likelihood framework back to our empirical risk minimization framework."
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/08c.conditional-probability-models.pdf"
  Notes:
    - {"Exponential Distribution Gradient Boosting (First part)": "https://davidrosenberg.github.io/mlcourse/Notes/conditional-exponential-distributions.pdf"}
    - {"Poisson Gradient Boosting (First part)": "https://davidrosenberg.github.io/mlcourse/Notes/poisson-gradient-boosting.pdf"}

  CChecks:
    - {Conditional Model Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/10-Lab-Check.pdf"}
    - {Conditional Model Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/10-Lab-Check_sol.pdf"}
    - {"Homework 5: §3": "#assignment-homework-5"}
  Video: "https://www.youtube.com/embed/JrFj0xpGd2Q"
-
  Title: "18. Bayesian Methods"
  Summary: "<p>We review some basics of classical and Bayesian statistics. For classical \"frequentist\" statistics, we define statistics and point estimators, and discuss various desirable properties of point estimators. For Bayesian statistics, we introduce the \"prior distribution\", which is a distribution on the parameter space that you declare before seeing any data. We compare the two approaches for the simple problem of learning about a coin's probability of heads. Along the way, we discuss conjugate priors, posterior distributions, and credible sets. Finally, we give the basic setup for Bayesian decision theory, which is how a Bayesian would go from a posterior distribution to choosing an action.</p>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/09a.bayesian-methods.pdf"
  Video: "https://www.youtube.com/embed/VCfrGjDPC6k"
  CChecks:
    - {Bayesian Methods and Regression Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/11-Lec-Check.pdf"}
    - {Bayesian Methods and Regression Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/11-Lec-Check_sol.pdf"}
    - {"Homework 5: §7,8": "#assignment-homework-5"}
  Notes:
    - {"Proportionality Review": "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Notes/proportionality.pdf"}
    - {"Thompson Sampling for Bernoulli Bandits [Optional]": "https://davidrosenberg.github.io/mlcourse/in-prep/thompson-sampling-bernoulli.pdf"}
-
  Title: "19. Bayesian Conditional Probability Models"
  Summary: "<p>In our earlier discussion of conditional probability modeling, we started with a hypothesis space of conditional probability models, and we selected a single conditional probability model using maximum likelihood or regularized maximum likelihood. In the Bayesian approach, we start with a prior distribution on this hypothesis space, and after observing some training data, we end up with a posterior distribution on the hypothesis space. For making conditional probability predictions, we can derive a predictive distribution from the posterior distribution. We explore these concepts by working through the case of Bayesian Gaussian linear regression. We also make a precise connection between MAP estimation in this model and ridge regression.</p>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/09b.bayesian-regression.pdf"
  Video: "https://www.youtube.com/embed/Mo4p2B37LwY"
  CChecks:
    - {Bayesian Methods and Regression Questions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/11-Lec-Check.pdf"}
    - {Bayesian Methods and Regression Solutions: "https://davidrosenberg.github.io/mlcourse/ConceptChecks/11-Lec-Check_sol.pdf"}
  Notes:
    - {"Proportionality Review": "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Notes/proportionality.pdf"}
  References:
    - "Barber 9.1, 18.1"
    - "Bishop 3.3"
-
  Title: "20. Classification and Regression Trees"
  Summary: "We begin our discussion of nonlinear models with tree models. We first describe the hypothesis space of decision trees, and we discuss some complexity measures we can use for regularization, including tree depth and the number of leaf nodes.  The challenge starts when we try to find the regularized empirical risk minimizer (ERM) over this space for some loss function. It turns out finding this ERM is computationally intractable. We discuss a standard greedy approach to tree building, both for classification and regression, in the case that features take values in any ordered set. We also describe an approach for handling categorical variables (in the binary classification case) and missing values."
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/10a.trees.pdf"
  Video: "https://www.youtube.com/embed/GZuweldJWrM"
  Notes:
    - {Categorical variables with trees (ipynb): "https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Contrib/trees/categorical_variables.ipynb"}
    - {Missing data and surrogate splits (ipynb): "https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Contrib/trees/surrogate_split.ipynb"}
  CChecks:
    - {"Homework 6: §6": "#assignment-homework-6"}
  References:
    - "JWHT 8.1"
    - "HTF 9.2"
    - {"CART book by Breiman et al.": "http://a.co/bNi2hH5"}
-
  Title: "21. Basic Statistics and a Bit of Bootstrap"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/10b.bootstrap.pdf"
  Summary: "<p>In this lecture, we define bootstrap sampling and show how it is typically applied in statistics to do things such as estimating variances of statistics and making confidence intervals.  It can be used in a machine learning context for assessing model performance.</p>"
  Video: "https://www.youtube.com/embed/lr5WH-JVT5I"
  References:
    - "JWHT 5.2 (Bootstrap)"
    - "HTF 7.11 (Bootstrap)"
-
  Title: "22. Bagging and Random Forests"
  Summary: "<p>We motivate bagging as follows: Consider the regression case, and suppose we could create a bunch (say B) prediction functions based on independent training samples of size n. If we average together these prediction functions, the expected value of the average is the same as any one of the functions, but the variance would have decreased by a factor of 1/B -- a clear win! Of course, this would require an overall sample of size nB. The idea of bagging is to replace independent samples with bootstrap samples from a single data set of size n. Of course, the bootstrap samples are not independent, so much of our discussion is about when bagging does and does not lead to improved performance.  Random forests were invented as a way to create conditions in which bagging works better.</p>
<details>
  <p><summary>More...</summary>Although it's hard to find crisp theoretical results describing when bagging helps, conventional wisdom says that it helps most for models that are \"high variance\", which in this context means the prediction function may change a lot when you train with a new random sample from the same distribution, and \"low bias\", which basically means fitting the training data well. Large decision trees have these characteristics and are usually the model of choice for bagging. Random forests are just bagged trees with one additional twist: only a random subset of features are considered when splitting a node of a tree. The hope, very roughly speaking, is that by injecting this randomness, the resulting prediction functions are less dependent, and thus we'll get a larger reduction in variance. In practice, random forests are one of the most effective machine learning models in many domains.</p></details>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/10c.bagging-random-forests.pdf"
  Video: "https://www.youtube.com/embed/f2S4hVs-ESw"
  Notes:
    - {"Trees, Bootstrap, Bagging, and RF Questions": "https://davidrosenberg.github.io/mlcourse/ConceptChecks/9-Lec-Check.pdf"}
    - {"Trees, Bootstrap, Bagging, and RF Solutions": "https://davidrosenberg.github.io/mlcourse/ConceptChecks/9-Lec-Check_sol.pdf"}
  References:
    - "JWHT 8.2"
    - "HTF 8.7, 15, 10"
    - {A Conversation with Jerry Friedman: "https://arxiv.org/pdf/1507.08502.pdf"}
-
  Title: "23. Gradient Boosting"
  Summary: "<p>Gradient boosting is an approach to \"adaptive basis function modeling\", in which we learn a linear combination of M basis functions, which are themselves learned from a base hypothesis space H.  Gradient boosting may be used with any subdifferentiable loss function and over any base hypothesis space on which we can do regression. Regression trees are the most commonly used base hypothesis space. It is important to note that the \"regression\" in \"gradient boosted regression trees\" (GBRTs) refers to how we fit the basis functions, not the overall loss function.  GBRTs are routinely used for classification and conditional probability modeling. They are among the most dominant methods in competitive machine learning (e.g. Kaggle competitions).</p>
<details>
  <p><summary>More...</summary>If the base hypothesis space H has a nice parameterization (say differentiable, in a certain sense), then we may be able to use standard gradient-based optimization methods directly. In fact, neural networks may be considered in this category. However, if the base hypothesis space H consists of trees, then no such parameterization exists.  This is where gradient boosting is really needed.</p>
  <p>For practical applications, it would be worth checking out the GBRT implementations in <a href=\"https://xgboost.ai/\">XGBoost</a> and <a href=\"https://github.com/Microsoft/lightGBM\">LightGBM</a>.</p>
  <p>See the Notes below for fully worked examples of doing gradient boosting for classification, using the hinge loss, and for conditional probability modeling using both exponential and Poisson distributions.  The code gbm.py illustrates L2-boosting and L1-boosting with decision stumps, for a one-dimensional regression dataset.</p>
</details>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/11a.gradient-boosting.pdf"
  Video: "https://www.youtube.com/embed/fz1H03ZKvLM"
  Notes:
    - {"Gradient Boosting Questions": "https://davidrosenberg.github.io/mlcourse/ConceptChecks/boosting-Lec-Check.pdf"}
    - {"Gradient Boosting Solutions": "https://davidrosenberg.github.io/mlcourse/ConceptChecks/boosting-Lec-Check_sol.pdf"}
    - {"Exponential Distribution Gradient Boosting": "https://davidrosenberg.github.io/mlcourse/Notes/conditional-exponential-distributions.pdf"}
    - {"Poisson Gradient Boosting": "https://davidrosenberg.github.io/mlcourse/Notes/poisson-gradient-boosting.pdf"}
    - {"gbm.py": "https://davidrosenberg.github.io/mlcourse/Archive/2017/Labs/gbm.py"}
  CChecks:
    - {"Homework 6: §7,8": "#assignment-homework-6"}
  References:
    - {"Friedman's GBM Paper": http://statweb.stanford.edu/~jhf/ftp/trebst.pdf}
    - {"Ridgeway's GBM Guide": http://www.saedsayad.com/docs/gbm2.pdf}
    - {XGBoost Paper: http://arxiv.org/abs/1603.02754}
#    - {"Bühlmann and Hothorn's Boosting Paper": https://projecteuclid.org/euclid.ss/1207580163}
-
  Title: "24. Multiclass and Introduction to Structured Prediction"
  Summary: "Here we consider how to generalize the score-producing binary classification methods we've discussed (e.g. SVM and logistic regression) to multiclass settings. We start by discussing \"One-vs-All\", a simple reduction of multiclass to binary classification. This usually works just fine in practice, despite the interesting failure case we illustrate. However, One-vs-All doesn't scale to a very large number of classes, since we have to train a separate model for each class. This is the real motivation for presenting the \"compatibility function\" approach described in this lecture. The approach presented here extends to structured prediction problems, where the output space may be exponentially large. We didn't have time to define structured prediction in the lecture, but please see the slides and the SSBD book in the references."
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/11b.multiclass.pdf"
  Video: "https://www.youtube.com/embed/WMQwtoMUjDA"
  CChecks:
    - {"Multiclass Questions": "https://davidrosenberg.github.io/mlcourse/ConceptChecks/7-Lec-Check.pdf"}
    - {"Multiclass Solutions": "https://davidrosenberg.github.io/mlcourse/ConceptChecks/7-Lec-Check_sol.pdf"}
    - {"Homework 6: §1-5": "#assignment-homework-6"}
  References:
    - "SSBD 17.1-17.3"
    - {In Defense of One-vs-All Classification: "http://www.jmlr.org/papers/v5/rifkin04a.html"}
    - {Reducing Multiclass to Binary: "http://www.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf"} 
-
  Title: "25. k-Means Clustering"
  Summary: "Here we start our short unit on unsupervised learning. k-means clustering is presented first as an algorithm and then as an approach to minimizing a particular objective function. One challenge with clustering algorithms is that it's not obvious how to measure success.  (See Section 22.5 of the SSBD book for a nice discussion.) When possible, I prefer to take a probabilistic modeling approach, as discussed in the next two lectures."
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13a.k-means.pdf"
  Video: "https://www.youtube.com/embed/J0A_tkIgutw"
  References:
    - "HTF 13.2.1"
    - "SSBD 22.2"
-
  Title: "26. Gaussian Mixture Models"
  Summary: "A Gaussian mixture model (GMM) is a family of multimodal probability distributions, which is a plausible generative model for clustered data. We can fit this model using maximum likelihood, and we can assess the quality of fit by evaluating the model likelihood on holdout data. While the \"learning\" phase of Gaussian mixture modeling is fitting the model to data, in the \"inference\" phase, we determine for any point drawn from the GMM the probability that it came from each of the k components. To use a GMM for clustering, we simply assign each point to the component that it is most likely to have come from. k-means clustering can be seen as a limiting case of a restricted form of Gaussian mixture modeling."
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13b.mixture-models.pdf"
  Video: "https://www.youtube.com/embed/I9dfOMAhsug"
  References:
    - "Bishop 9.2,9.3"
    - {"An Alternative to EM for GMM [Optional]": "https://arxiv.org/pdf/1706.03267.pdf"}
-
  Title: "27. EM Algorithm for Latent Variable Models"
  Summary: "It turns out, fitting a Gaussian mixture model by maximum likelihood is easier said than done: there is no closed from solution, and our usual gradient methods do not work well. The standard approach to maximum likelihood estimation in a Gaussian mixture model is the expectation maximization (EM) algorithm. In this lecture, we present the EM algorithm in the general setting of latent variable models, of which GMM is a special case. We present the EM algorithm as a very basic \"variational method\" and indicate a few generalizations."
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/13c.EM-algorithm.pdf"
  Video: "https://www.youtube.com/embed/lMShR1vjbUo"
  References:
    - "Bishop 9.4"
    - {"Vaida's \"Parameter Convergence for EM and MM Algorithms\"": "http://www3.stat.sinica.edu.tw/statistica/oldpdf/a15n316.pdf"}
-
  Title: "28. Neural Networks"
  Summary: "In the context of this course, we view neural networks as \"just\" another nonlinear hypothesis space. On the practical side, unlike trees and tree-based ensembles (our other major nonlinear hypothesis spaces), neural networks can be fit using gradient-based optimization methods. On the theoretical side, a large enough neural network can approximate any continuous function. We discuss the specific case of the multilayer perceptron for multiclass classification, which we view as a generalization of multinomial logistic regression from linear to nonlinear score functions."
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/14a.neural-networks.pdf"
  Video: "https://www.youtube.com/embed/Wr11D5sObzc"
  References:
    - {"Michael Nielsen's chapter on universality of neural networks": "http://neuralnetworksanddeeplearning.com/chap4.html"}
-
  Title: "29. Backpropagation and the Chain Rule"
  Summary: "<p>Neural network optimization is amenable to gradient-based methods, but if the actual computation of the gradient is done naively, the computational cost can be prohibitive. Backpropagation is the standard algorithm for computing the gradient efficiently. We present the backpropagation algorithm for a general computation graph. The algorithm we present applies, without change, to models with \"parameter tying\", which include convolutional networks and recurrent neural networks (RNN's), the workhorses of modern computer vision and natural language processing. We illustrate backpropagation with one of the simplest models with parameter tying: regularized linear regression. Backpropagation for the multilayer perceptron, the standard introductory example, is presented in detail in <a href=\"#assignment-homework-7\">Hwk 7 Problem 4</a>.</p>"
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/14b.backpropagation.pdf"
  Video: "https://www.youtube.com/embed/XIpyEvLv93A"
  CChecks:
    - {"Homework 7": "#assignment-homework-7"}
  References:
    - {'Yes you should understand backprop (Karpathy)': "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b"}
    - {'Challenges with backprop (Karpathy Lecture)': "https://youtu.be/gYpoJMlgyXA?t=13m44s"}
-
  Title: "30. Next Steps"
  Summary: "We point the direction to many other topics in machine learning that should be accessible to students of this course, but that we did not have time to cover."
  Slides: "https://davidrosenberg.github.io/mlcourse/Archive/2017Fall/Lectures/14c.next-steps.pdf"
  Video: "https://www.youtube.com/embed/RMmAVrhAfWs"
